window.topicHeading = 'Names & Concepts: Christian, The Alignment Problem';
window.flashcardData = [
  {
    question: "Dario Amodei's 'boat'",
    answer: "CoastRunners reward hacking: an RL agent learns to loop in the harbor, repeatedly hitting targets/obstacles to farm points instead of finishing the race—classic specification gaming.",
    tags: ["Christian"]
  },
  {
    question: "near- vs long-term AI risks",
    answer: "Near-term: bias, privacy, transparency, labor impacts, misuse. Long-term: control/alignment of powerful systems, safety, catastrophic/existential risk.",
    tags: ["Christian"]
  },
  {
    question: "Minsky & Papert Perceptrons (1969)",
    answer: "Book proving limits of single-layer perceptrons (e.g., can't learn XOR), helping trigger an early 'AI winter' until multi-layer methods returned.",
    tags: ["Christian"]
  },
  {
    question: "Amazon Mechanical Turk",
    answer: "Crowd sourcing platform for human annotation and microtasks; widely used to label datasets that train ML systems.",
    tags: ["Christian"]
  },
  {
    question: "CIFAR-10",
    answer: "Benchmark dataset of 60k 32x32 color images across 10 classes; standard testbed for vision models.",
    tags: ["Christian"]
  },
  {
    question: "ImageNet",
    answer: "Large-scale labeled image dataset (millions of images, 1k+ categories) behind the ImageNet Challenge that catalyzed the deep-learning boom.",
    tags: ["Christian"]
  },
  {
    question: "Joy Buolamwini",
    answer: "Researcher/advocate who exposed accuracy gaps in commercial face analysis (Gender Shades); founder of Algorithmic Justice League.",
    tags: ["Christian"]
  },
  {
    question: "Shirley Card",
    answer: "Kodak's light-skin calibration standard for color film—an example of built-in bias shaping imaging tech and downstream datasets.",
    tags: ["Christian"]
  },
  {
    question: "N-grams",
    answer: "Sequences of N tokens; a simple language model counts their frequencies to predict next words.",
    tags: ["Christian"]
  },
  {
    question: "curse of dimensionality",
    answer: "As features grow, data become sparse and distances less informative; learning/reasoning gets harder without dimensionality reduction.",
    tags: ["Christian"]
  },
  {
    question: "word embedding",
    answer: "Dense vector representation of words so semantic similarity corresponds to geometric closeness.",
    tags: ["Christian"]
  },
  {
    question: "corpus",
    answer: "A large body of text used to train or evaluate language models.",
    tags: ["Christian"]
  },
  {
    question: "distributional hypothesis",
    answer: "“You shall know a word by the company it keeps”: meaning inferred from contextual co-occurrence.",
    tags: ["Christian"]
  },
  {
    question: "principal component analysis (PCA)",
    answer: "Linear technique projecting data onto orthogonal directions that capture maximal variance—used to reduce dimensions.",
    tags: ["Christian"]
  },
  {
    question: "perceptron",
    answer: "A linear threshold classifier with a simple learning rule; a foundational (but limited) neural model.",
    tags: ["Christian"]
  },
  {
    question: "training set",
    answer: "Portion of data used to fit model parameters (distinct from validation/test for evaluation).",
    tags: ["Christian"]
  },
  {
    question: "Labeled Faces in the Wild (LFW)",
    answer: "Unconstrained face-photo benchmark for verification/recognition; exposes real-world variability.",
    tags: ["Christian"]
  },
  {
    question: "debiasing",
    answer: "Methods to reduce harmful bias in data/models (e.g., re-weighting, adversarial training, embedding neutralization).",
    tags: ["Christian"]
  },
    {
    question: 'Alex Krizhevsky',
    answer: 'Co-author of AlexNet; led the 2012 ImageNet breakthrough that ignited the deep-learning boom.',
    tags: ['Christian']
  },
  {
    question: 'Geoffrey Hinton',
    answer: 'Deep-learning pioneer and back propagation evangelist; mentor to the AlexNet team; 2018 Turing Award.',
    tags: ['Christian']
  },
  {
    question: 'Ilya Sutskever',
    answer: 'AlexNet co-author; co-founded OpenAI; key work on sequence-to-sequence learning at Google Brain.',
    tags: ['Christian']
  },
  {
    question: 'word2vec',
    answer: 'Mikolov et al. (2013) algorithm that learns dense word embeddings capturing semantic relations.',
    tags: ['Christian']
  },
  {
    question: 'COMPAS',
    answer: 'Proprietary recidivism risk score used in U.S. courts; central to debates about algorithmic bias.',
    tags: ['Christian']
  },
    {
    question: "Alignment Problem",
    answer: "The challenge of ensuring AI systems do what humans intend—not just what we explicitly tell them. E.g., boat goes for points not proper racing.",
    tags: ["Christian"]
  },
  {
    question: "Rewarding A while hoping for B",
    answer: "A classic misalignment failure: systems optimize for measurable proxies that don't match human intentions.",
    tags: ["Christian"]
  },
  {
    question: "Machine Learning (ML)",
    answer: "A method where computers learn patterns from data, instead of being explicitly programmed.",
    tags: ["Christian"]
  },
  {
    question: "Unsupervised Learning",
    answer: "A type of ML where the system finds patterns in data without labeled outcomes. E.g., learning word associations from text corpora.",
    tags: ["Christian"]
  },
  {
    question: "Supervised Learning",
    answer: "ML that uses labeled data (input-output pairs) to learn a predictive function. E.g., learning behavior prediction from historical records.",
    tags: ["Christian"]
  },
  {
    question: "Reinforcement Learning (RL)",
    answer: "ML where an agent learns by trial and error in an environment with rewards and penalties.",
    tags: ["Christian"]
  },
  {
    question: "Word Embeddings",
    answer: "Numerical representations of words capturing their meanings based on context. E.g., king - man + woman = queen",
    tags: ["Christian"]
  },
  {
    question: "word2vec",
    answer: "A specific algorithm that learns word embeddings from large text datasets.",
    tags: ["Christian"]
  },
  {
    question: "Bias in AI",
    answer: "Systematic errors or unfair outcomes resulting from data, models, or assumptions. E.g., resume bot prefers male-coded words.",
    tags: ["Christian"]
  },
  {
    question: "Proxy Variable",
    answer: "A stand-in metric used when the true goal is hard to measure. E.g., points instead of playing game well.",
    tags: ["Christian"]
  },
  {
    question: "Objective Function",
    answer: "The formal specification of what an AI should optimize. E.g., the score in a game.",
    tags: ["Christian"]
  },
  {
    question: "Neural Network",
    answer: "A computing model inspired by the brain, used to identify patterns in data.",
    tags: ["Christian"]
  },
  {
    question: "Perceptron",
    answer: "An early neural network model that could learn to recognize patterns.",
    tags: ["Christian"]
  },
  {
    question: "Stochastic Gradient Descent (SGD)",
    answer: "A method for adjusting weights in a neural network in the direction of minimizing error.",
    tags: ["Christian"]
  },
  {
    question: "Risk-Assessment Algorithms",
    answer: "Tools that predict outcomes like recidivism to assist real-world decision-making.",
    tags: ["Christian"]
  },
  {
    question: "Black Box Model",
    answer: "A system whose internal workings are opaque or difficult to interpret. We just know the inputs and the outputs.",
    tags: ["Christian"]
  },
  {
    question: "Debiasing",
    answer: "Techniques used to reduce or remove unwanted biases in AI models. E.g., tweaking word embeddings to remove gender bias.",
    tags: ["Christian"]
  },
  {
    question: "Principal Component Analysis (PCA)",
    answer: "A technique to identify the most important dimensions in high-dimensional data.",
    tags: ["Christian"]
  },
  {
    question: "Analogical Reasoning (in AI)",
    answer: "The ability to draw comparisons or analogies between concepts like when AI infers \"Rome\" from \"Paris - France + Italy\".",
    tags: ["Christian"]
  }
];
